# model.py - ç»ˆæä¿®å¤ç‰ˆ BriLLM0.5 (æ¨¡æ‹Ÿå¤§è„‘æ¨ç†æ¶æ„)
import torch
import torch.nn as nn
import random

class BraLM(nn.Module):
    def __init__(self, hidden_size=512, embed_dim=256, rank=8, use_ds=False, vocab=None):
        """
        æ¨¡æ‹Ÿå¤§è„‘æ¨ç†æ¶æ„çš„è½»é‡ç‰ˆ BriLLM0.5
        ä¸“ä¸ºè§£å†³å›¾ç»“æ„è¯­è¨€æ¨¡å‹çš„ç»´åº¦é—®é¢˜è®¾è®¡
        """
        super().__init__()
        self.hidden_size = hidden_size
        self.embed_dim = embed_dim
        self.rank = rank
        self.activation = nn.GELU()
        self.positions = nn.Parameter(torch.ones(1, 512, 1))
        self.device = None
        self.use_ds = use_ds
        self.vocab = vocab

        # æ ¸å¿ƒï¼šç”¨åµŒå…¥ + ä½ç§©å˜æ¢æ›¿ä»£å·¨å¤§å‚æ•°çŸ©é˜µ
        self.node_embeddings = nn.Embedding(vocab.num_nodes, embed_dim)
        self.W_left = nn.Linear(embed_dim, hidden_size * rank)
        self.W_right = nn.Linear(embed_dim, hidden_size * rank)
        self.bias_base = nn.Parameter(torch.zeros(1, 1, hidden_size))
        self.node_bias = nn.Embedding(vocab.num_nodes, hidden_size)

        # å‚æ•°åˆå§‹åŒ–
        nn.init.normal_(self.node_embeddings.weight, std=0.02)
        nn.init.normal_(self.bias_base, std=0.02)
        nn.init.normal_(self.node_bias.weight, std=0.02)

    def prepare_network(self, vocab):
        self.vocab = vocab
        print(f"ğŸ§  æ¨¡å‹å·²å‡†å¤‡ï¼ŒèŠ‚ç‚¹æ•°: {vocab.num_nodes}, éšè—å±‚: {self.hidden_size}")

    def to_device(self, device):
        self.device = device
        self = self.to(device)

    def get_positional_encoding(self, seq_len, d_model):
        """ç”Ÿæˆæ­£å¼¦ä½ç½®ç¼–ç """
        position = torch.arange(0, seq_len).unsqueeze(1)
        div_term = 10000.0 ** (torch.arange(0, d_model, 2) / d_model)
        pe = torch.zeros(seq_len, d_model)
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).to(self.device)
        if self.node_embeddings.weight.dtype == torch.float16:
            pe = pe.half()
        return pe

    def get_initial_tensor(self, batch_size, src_node_idx, pe, pos=0):
        """åˆå§‹åŒ–èƒ½é‡å¼ é‡"""
        node_emb = self.node_embeddings(src_node_idx[:, 0])
        energy_tensor = self.activation(self.node_bias(src_node_idx[:, 0]))
        energy_tensor = energy_tensor.unsqueeze(1)
        pe = pe[:, pos] if pe.dim() == 3 else pe
        return self.activation(energy_tensor + pe[:batch_size])

    def forward(self, neighbor_ids):
        """
        å‰å‘ä¼ æ’­ - ä¿®å¤äº†ç»´åº¦é—®é¢˜
        :param neighbor_ids: (bs, seq_len, 1+k, 2)  # 2 è¡¨ç¤º (src_idx, tgt_idx)
        """
        batch_size, seq_len = neighbor_ids.shape[:2]
        device = neighbor_ids.device
        self.to_device(device)

        pe = self.get_positional_encoding(512, self.hidden_size)
        energy_cache = []
        loss = 0.0

        for i in range(seq_len):
            d = neighbor_ids[:, i]
            src_idx = d[..., 0]
            tgt_idx = d[..., 1]
            num_neg_samples = tgt_idx.size(1)  # è·å–è´Ÿæ ·æœ¬æ•° (1+k)

            if i == 0:
                energy_tensor = self.get_initial_tensor(batch_size, src_idx, pe, 0)
            else:
                cache_tensor = torch.stack(energy_cache, dim=1)
                weights = self.positions[:, :i].softmax(dim=1)
                energy_tensor = (cache_tensor * weights).sum(dim=1, keepdim=True)
                
                # ç¡®ä¿ energy_tensor æ˜¯ 3D [bs, 1, D]
                if energy_tensor.dim() > 3:
                    energy_tensor = energy_tensor.squeeze(2)

            # ç¡®ä¿ energy_tensor æ˜¯ 3D [bs, 1, D]
            if energy_tensor.dim() == 2:
                energy_tensor = energy_tensor.unsqueeze(1)
            elif energy_tensor.dim() == 1:
                energy_tensor = energy_tensor.unsqueeze(0).unsqueeze(0)
            # å¦‚æœå·²ç»æ˜¯ 4Dï¼Œå»æ‰å¤šä½™çš„ç»´åº¦
            elif energy_tensor.dim() > 3:
                energy_tensor = energy_tensor.squeeze(2)

            # è·å–èŠ‚ç‚¹åµŒå…¥
            src_emb = self.node_embeddings(src_idx)
            tgt_emb = self.node_embeddings(tgt_idx)

            # ç”Ÿæˆä½ç§©æƒé‡çŸ©é˜µ W âˆˆ R^(bs*(1+k), D, D)
            U = self.W_left(src_emb).view(-1, self.rank, self.hidden_size)
            V = self.W_right(tgt_emb).view(-1, self.rank, self.hidden_size)
            W = torch.bmm(U.transpose(1, 2), V) / (self.rank ** 0.5)

            # ç”Ÿæˆåç½® b âˆˆ R^(bs, 1+k, 1, D)
            bias = self.bias_base + self.node_bias(tgt_idx).unsqueeze(2)

            # å…³é”®ä¿®å¤ï¼šæ­£ç¡®æ‰©å±• energy_tensor
            # (bs, 1, D) -> (bs, 1+k, 1, D) -> (bs*(1+k), 1, D)
            expand_energy = energy_tensor.unsqueeze(1).expand(-1, num_neg_samples, -1, -1)
            expand_energy = expand_energy.reshape(-1, 1, self.hidden_size)

            nxt_energy = torch.bmm(expand_energy, W) + bias.reshape(-1, 1, self.hidden_size)
            nxt_energy = self.activation(nxt_energy + pe[:, i+1].unsqueeze(0))

            output_tensor = nxt_energy.reshape(batch_size, -1, 1, self.hidden_size)

            if i == 0:
                energy_cache = [output_tensor[:, 0]]
            else:
                energy_cache.append(output_tensor[:, 0])

            energy = output_tensor.norm(p=2, dim=(-2, -1))
            label = torch.zeros(batch_size, dtype=torch.long, device=device)
            loss += nn.CrossEntropyLoss()(energy, label)

        return loss / seq_len

    def decode(self, start, vocab, max_new_tokens=16, do_sample=False, temperature=1.0):
        """
        æ¨ç†ç”Ÿæˆ - ä¿®å¤äº†èµ·å§‹ç‚¹é—®é¢˜
        :param start: èµ·å§‹è¾¹åˆ—è¡¨ï¼Œå¦‚ [(0,1), (1,2)]
        :return: ç”Ÿæˆçš„è¾¹åºåˆ—
        """
        ret = []
        device = next(self.parameters()).device
        self.to_device(device)
        pe = self.get_positional_encoding(512, self.hidden_size).squeeze(0)
        energy_cache = []

        for i, (s, t) in enumerate(start):
            if i == 0:
                src_idx = torch.tensor([[s]], device=device)
                energy_tensor = self.get_initial_tensor(1, src_idx, pe.unsqueeze(0), 0).squeeze(0)
            else:
                cache_tensor = torch.stack(energy_cache, dim=1)
                weights = self.positions[:, :i].softmax(dim=1)
                energy_tensor = (cache_tensor * weights).sum(dim=1, keepdim=True)
                
                # ç¡®ä¿ energy_tensor æ˜¯ 2D [1, D]
                if energy_tensor.dim() > 2:
                    energy_tensor = energy_tensor.squeeze(0).squeeze(0)
                elif energy_tensor.dim() == 2:
                    energy_tensor = energy_tensor.squeeze(0)

            # è·å–åµŒå…¥
            s_emb = self.node_embeddings(torch.tensor([s], device=device))
            t_emb = self.node_embeddings(torch.tensor([t], device=device))
            U = self.W_left(s_emb).view(self.rank, self.hidden_size)
            V = self.W_right(t_emb).view(self.rank, self.hidden_size)
            W = torch.mm(U.transpose(0, 1), V) / (self.rank ** 0.5)
            b = self.bias_base + self.node_bias(torch.tensor([t], device=device)).unsqueeze(1)

            if energy_tensor.dim() == 1:
                energy_tensor = energy_tensor.unsqueeze(0)

            nxt_energy = torch.mm(energy_tensor, W) + b.squeeze(0)
            energy_tensor = self.activation(nxt_energy + pe[i+1].unsqueeze(0)).squeeze(0)

            if i == 0:
                energy_cache = [energy_tensor.unsqueeze(0).unsqueeze(0)]
            else:
                energy_cache.append(energy_tensor.unsqueeze(0).unsqueeze(0))

            ret.append((s, t))

        x = t
        for i in range(max_new_tokens):
            candidates = vocab.get_neighbor_of_node(x, -1)
            if not candidates:
                break

            # å¤„ç†å€™é€‰èŠ‚ç‚¹
            candidate_indices = []
            for c in candidates:
                if "->" in c:
                    _, t_str = c.split("->", 1)
                    t_idx = vocab.node_dict.get(t_str)
                    if t_idx is not None:
                        candidate_indices.append(t_idx)
            
            if not candidate_indices:
                break
                
            tgt_indices = torch.tensor(candidate_indices, device=device)
            x_emb = self.node_embeddings(torch.tensor([x], device=device))
            t_embs = self.node_embeddings(tgt_indices)

            U = self.W_left(x_emb).view(self.rank, self.hidden_size)
            V = self.W_right(t_embs).view(-1, self.rank, self.hidden_size)
            W = torch.bmm(U.unsqueeze(0).expand(len(t_embs), -1, -1).transpose(1, 2), V) / (self.rank ** 0.5)
            b = self.bias_base + self.node_bias(tgt_indices).unsqueeze(1)

            curr_i = len(start) + i
            current_energy = (torch.cat(energy_cache, dim=1) * self.positions[:, :curr_i].softmax(1)).sum(1)
            
            # ç¡®ä¿ current_energy æ˜¯ 2D [1, D]
            if current_energy.dim() > 2:
                current_energy = current_energy.squeeze(0)
            elif current_energy.dim() == 1:
                current_energy = current_energy.unsqueeze(0)
                
            expand_energy = current_energy.unsqueeze(1).repeat(len(W), 1, 1)

            if expand_energy.dim() == 2:
                expand_energy = expand_energy.unsqueeze(1)

            nxt_energy = torch.bmm(expand_energy, W) + b
            output_tensor = self.activation(nxt_energy + pe[curr_i+1].unsqueeze(0).unsqueeze(0))
            energy = output_tensor.norm(p=2, dim=(1, 2))
            probs = torch.softmax(energy / (temperature + 1e-8), dim=-1)

            if do_sample:
                index = torch.multinomial(probs, 1).item()
            else:
                index = probs.argmax().item()

            # ä» candidates ä¸­è·å–ä¸‹ä¸€ä¸ªèŠ‚ç‚¹
            if "->" in candidates[index]:
                _, next_node_str = candidates[index].split("->", 1)
                y = vocab.node_dict.get(next_node_str, x)
            else:
                y = x

            ret.append((x, y))
            energy_cache.append(output_tensor[index].unsqueeze(0))
            x = y

        return ret


class Vocab:
    def __init__(self, node_dict, nodeindex_dict, edge_dict, edge_decode_dict):
        self.node_dict = node_dict
        self.nodeindex_dict = nodeindex_dict
        self.edge_dict = edge_dict
        self.edge_decode_dict = edge_decode_dict

        # å¤„ç† None å€¼
        if self.nodeindex_dict is None:
            self.nodeindex_dict = {}
            node_id = 0
            for s in self.edge_dict:
                if s == "" or s not in self.nodeindex_dict.values():
                    self.nodeindex_dict[node_id] = s
                    node_id += 1
        if self.node_dict is None:
            self.node_dict = {v: k for k, v in self.nodeindex_dict.items()}

        self.num_nodes = len(self.nodeindex_dict)

    @classmethod
    def from_edge(cls, filename):
        edge_dict = {"": {"": (0, 0)}}
        edge_decode_dict = {(0, 0): ""}

        with open(filename, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line or line.startswith('#') or line.startswith('//'):
                    continue
                if '->' not in line:
                    print(f"âš ï¸ æ— æ•ˆè¡Œ {line_num}: '{line}' (ç¼ºå°‘ '->')")
                    continue

                try:
                    s, t = line.split('->', 1)
                    s, t = s.strip(), t.strip()
                    if not s or not t:
                        print(f"âš ï¸ èŠ‚ç‚¹ä¸ºç©º {line_num}: '{s}' -> '{t}'")
                        continue

                    if s not in edge_dict:
                        index_s = len(edge_dict)
                        edge_dict[s] = {}
                    else:
                        index_s = next(iter(edge_dict[s].values()))[0]

                    if t not in edge_dict[s]:
                        index_t = len(edge_dict[s])
                    else:
                        index_t = edge_dict[s][t][1]

                    edge_dict[s][t] = (index_s, index_t)
                    edge_decode_dict[(index_s, index_t)] = f"{s}->{t}"

                except Exception as e:
                    print(f"è§£æè¡Œ {line_num} å¤±è´¥: {line} | é”™è¯¯: {e}")
                    continue

        # æ„å»º node_dict å’Œ nodeindex_dict
        node_dict = {}
        nodeindex_dict = {}
        seen_nodes = set()

        for s in edge_dict:
            if s == "":
                continue
            seen_nodes.add(s)
            for t in edge_dict[s]:
                idx_s = edge_dict[s][t][0]
                node_dict[s] = idx_s
                nodeindex_dict[idx_s] = s
                break

        if "" not in node_dict:
            node_dict[""] = 0
            nodeindex_dict[0] = ""

        print(f"âœ… æˆåŠŸåŠ è½½ {len(seen_nodes)} ä¸ªèŠ‚ç‚¹ï¼Œ{len(edge_decode_dict)} æ¡è¾¹")
        return cls(node_dict, nodeindex_dict, edge_dict, edge_decode_dict)

    def get_neighbor_of_node(self, key, k):
        s = self.nodeindex_dict.get(key)
        if not s or s not in self.edge_dict:
            return []
        neighbors = [f"{s}->{t}" for t in self.edge_dict[s].keys() if t != s]
        random.shuffle(neighbors)
        return neighbors[:k] if k != -1 else neighbors

    def decode(self, x):
        return self.edge_decode_dict.get(x, f"UNK_{x}")

    def __call__(self, x):
        if isinstance(x, list):
            return [self.__call__(_) for _ in x]
        else:
            return self.fetch(x)

    def fetch(self, x):
        s, t = x.split("->")
        return self.edge_dict.get(s, {}).get(t, (0, 0))